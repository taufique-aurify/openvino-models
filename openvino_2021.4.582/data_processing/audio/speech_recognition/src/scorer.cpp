// Copyright (C) 2019-2020 Intel Corporation
// SPDX-License-Identifier: Apache-2.0
//

#include "scorer.h"

#include <chrono>
#include <thread>
#include <gna/gna_config.hpp>
#include <inference_engine.hpp>
#include "speech_parameters.h"
#include "logger.h"

using namespace InferenceEngine;

typedef std::chrono::high_resolution_clock Time;
typedef std::chrono::duration<double, std::ratio<1, 1000>> ms;

SpeechLibraryStatus Scorer::Initialize(const ScorerParameters& parameters)
{
    auto inference_version = GetInferenceEngineVersion();

    Logger::Print(LOG_LEVEL_INFO, "InferenceEngine %s ver. %d.%d (build: %s)",
        inference_version->description, inference_version->apiVersion.major,
        inference_version->apiVersion.minor, inference_version->buildNumber);

    context_window_left_ = parameters.context_window_left;
    context_window_right_ = parameters.context_window_right;

    batch_size_ = (context_window_left_ > 0 || context_window_right_ > 0) ?
        1 : static_cast<uint32_t>(parameters.batch_size);

    auto status = LoadInferenceEngine(std::string(parameters.infer_device));
    if (status != SPEECH_LIBRARY_SUCCESS)
    {
        return status;
    }

    // --------------------------- 2. Read IR Generated by ModelOptimizer (.xml and .bin files) ------------
    Core ie;
    CNNNetwork network = ie.ReadNetwork(parameters.model_network_path, parameters.model_weights_path);
    // -------------------------------------------------------------------------------------------------

    // --------------------------- 3. Set batch size ---------------------------------------------------
    /** Set batch size.  Unlike in imaging, batching in time (rather than space) is done for speech recognition. **/
    network.setBatchSize(batch_size_);
    Logger::Print(LOG_LEVEL_INFO, "Batch size: %zu", network.getBatchSize());

    /** Setting parameter for per layer metrics **/
    std::map<std::string, std::string> gnaPluginConfig;
    std::map<std::string, std::string> genericPluginConfig;
    if (use_gna_)
    {
        std::string infer_device = parameters.infer_device;
        std::string gnaDevice = use_hetero_ ?
            infer_device.substr(infer_device.find("GNA"), infer_device.find(",") - infer_device.find("GNA")) : infer_device;
        gnaPluginConfig[GNAConfigParams::KEY_GNA_DEVICE_MODE] =
            gnaDevice.find("_") == std::string::npos ? "GNA_AUTO" : gnaDevice;
    }

    gnaPluginConfig[GNA_CONFIG_KEY(SCALE_FACTOR)] = std::to_string(parameters.scale_factor);

    if (parameters.quantization_bits == 8) {
        gnaPluginConfig[GNAConfigParams::KEY_GNA_PRECISION] = "I8";
    }
    else if (parameters.quantization_bits == 16) {
        gnaPluginConfig[GNAConfigParams::KEY_GNA_PRECISION] = "I16";
    }
    else {
        Logger::Print(LOG_LEVEL_ERROR,
            "Invalid value of parameter '-inference:quantizationBits'. Valid range {8, 16}.");
        return SPEECH_LIBRARY_ERROR_INVALID_PARAM;
    }

    gnaPluginConfig[GNAConfigParams::KEY_GNA_LIB_N_THREADS] =
        std::to_string((context_window_left_ > 0 || context_window_right_ > 0) ?
            1 : parameters.infer_num_threads);
    gnaPluginConfig[GNA_CONFIG_KEY(COMPACT_MODE)] = CONFIG_VALUE(NO);
    // -----------------------------------------------------------------------------------------------------

    // --------------------------- 5. Loading model to the device ------------------------------------------

    if (use_gna_) {
        genericPluginConfig.insert(std::begin(gnaPluginConfig), std::end(gnaPluginConfig));
    }
    auto t0 = Time::now();

    executable_network_ = inference_engine_.LoadNetwork(network, device_name_, genericPluginConfig);

    ms loadTime = std::chrono::duration_cast<ms>(Time::now() - t0);
    Logger::Print(LOG_LEVEL_INFO, "Model loading time: %.2f ms", loadTime.count());

    // --------------------------- 7. Prepare input blobs --------------------------------------------------

    infer_requests_.resize((context_window_left_ > 0 || context_window_right_ > 0) ?
        1 : parameters.infer_num_threads);
    for (auto& inferRequest : infer_requests_) {
        inferRequest = { executable_network_.CreateInferRequest(), -1, batch_size_ };
    }
    /** Taking information about all topology inputs **/
    c_input_info_ = executable_network_.GetInputsInfo();

    InputsDataMap inputInfo;
    inputInfo = network.getInputsInfo();
    if (inputInfo.begin() != inputInfo.end())
    {
        if (nullptr == inputInfo.begin()->second)
        {
            Logger::Print(LOG_LEVEL_ERROR, "Input info is null");
            return SPEECH_LIBRARY_ERROR_INVALID_PARAM;
        }
        else
        {
            input_vector_size_ = inputInfo.begin()->second->getTensorDesc().getDims()[1];

            /** Stores all input blobs data **/
            if (c_input_info_.size() != 1) {
                Logger::Print(LOG_LEVEL_ERROR, "Sample supports only topologies with  1 input");
                return SPEECH_LIBRARY_ERROR_INVALID_PARAM;
            }

            /** configure input precision if model loaded from IR **/
            for (auto &item : inputInfo) {
                Precision inputPrecision = Precision::FP32;  // specify Precision::I16 to provide quantized inputs
                item.second->setPrecision(inputPrecision);
                item.second->getInputData()->setLayout(Layout::NC);  // row major layout
            }
        }
    }
    else
    {
        Logger::Print(LOG_LEVEL_ERROR, "Map of input info is empty");
        return SPEECH_LIBRARY_ERROR_INVALID_PARAM;
    }
    // -----------------------------------------------------------------------------------------------------

    // --------------------------- 8. Prepare output blobs -------------------------------------------------
    c_output_info_ = executable_network_.GetOutputsInfo();
    OutputsDataMap outputInfo;
    outputInfo = network.getOutputsInfo();
    if (outputInfo.begin() != outputInfo.end())
    {
        if (nullptr == outputInfo.begin()->second)
        {
            Logger::Print(LOG_LEVEL_ERROR, "Output info is null");
            return SPEECH_LIBRARY_ERROR_INVALID_PARAM;
        }
        else
        {
            output_vector_size_ = outputInfo.begin()->second->getDims()[1];

            for (auto &item : outputInfo) {
                DataPtr outData = item.second;
                if (!outData) {
                    Logger::Print(LOG_LEVEL_ERROR, "Failed to initialize scorer. Output data pointer is not valid");
                    return SPEECH_LIBRARY_ERROR_GENERIC;
                }

                Precision outputPrecision = Precision::FP32;  // specify Precision::I32 to retrieve quantized outputs
                outData->setPrecision(outputPrecision);
                outData->setLayout(Layout::NC);  // row major layout
            }
        }
    }
    else
    {
        Logger::Print(LOG_LEVEL_ERROR, "Map of input info is empty");
        return SPEECH_LIBRARY_ERROR_INVALID_PARAM;
    }
    // -----------------------------------------------------------------------------------------------------

    return SPEECH_LIBRARY_SUCCESS;
}

SpeechLibraryStatus Scorer::LoadInferenceEngine(const std::string& device_name)
{
    std::vector<std::string> possible_device_types = {
        "CPU",
        "GPU",
        "GNA_AUTO",
        "GNA_HW",
        "GNA_SW_EXACT",
        "GNA_SW",
        "GNA_SW_FP32",
        "HETERO:GNA,CPU",
        "HETERO:GNA_HW,CPU",
        "HETERO:GNA_SW_EXACT,CPU",
        "HETERO:GNA_SW,CPU",
        "MYRIAD"
    };

    if (std::find(possible_device_types.begin(), possible_device_types.end(), device_name)
        == possible_device_types.end())
    {
        Logger::Print(LOG_LEVEL_ERROR, "Specified device '%s' is not supported.", device_name.c_str());
        return SPEECH_LIBRARY_ERROR_INVALID_PARAM;
    }

    auto is_feature = [&](const std::string xFeature)
    {
        return device_name.find(xFeature) != std::string::npos;
    };

    use_gna_ = is_feature("GNA");
    use_hetero_ = is_feature("HETERO");
    device_name_ = use_hetero_ && use_gna_ ?
        "HETERO:GNA,CPU" : device_name.substr(0, (device_name.find("_")));

    auto device_versions = inference_engine_.GetVersions(device_name_);

    Logger::Print(LOG_LEVEL_INFO, "Device info:");
    for (auto& version : device_versions) {
        Logger::Print(LOG_LEVEL_INFO, "\t%s: %s ver. %d.%d",
            version.first.c_str(),
            version.second.description,
            version.second.apiVersion.major,
            version.second.apiVersion.minor);
    }

    return SPEECH_LIBRARY_SUCCESS;
}

SpeechLibraryStatus Scorer::ProcessData(const float* input, float* output, size_t number_of_frames)
{
    try {
        const float* input_index = input;
        float* output_index = output;

        size_t frame_index = 0;
        number_of_frames += context_window_left_ + context_window_right_;
        uint32_t frames_to_compute_in_this_batch = batch_size_;

        while (frame_index <= number_of_frames)
        {
            if (frame_index == number_of_frames)
            {
                if (std::find_if(infer_requests_.begin(), infer_requests_.end(),
                    [&](InferRequestStruct x) { return (x.frame_index != -1); })
                    == infer_requests_.end())
                {
                    break;
                }
            }

            for (auto &infer_request : infer_requests_)
            {
                if (frame_index == number_of_frames)
                {
                    frames_to_compute_in_this_batch = 1;
                }
                else {
                    frames_to_compute_in_this_batch = (number_of_frames - frame_index < batch_size_) ?
                        (number_of_frames - frame_index) : batch_size_;
                }

                if (infer_request.frame_index != -1)
                {
                    StatusCode code = infer_request.infer_request.Wait(
                        InferenceEngine::IInferRequest::WaitMode::RESULT_READY);
                    if (code != StatusCode::OK)
                    {
                        if (!use_hetero_) continue;
                        if (code != StatusCode::INFER_NOT_STARTED) continue;
                    }

                    if (infer_request.frame_index >= 0)
                    {
                        if (c_output_info_.begin() == c_output_info_.end())
                        {
                            Logger::Print(LOG_LEVEL_ERROR, "Inference has no single output info element");
                            return SPEECH_LIBRARY_ERROR_GENERIC;
                        }
                        else
                        {
                            Blob::Ptr outputBlob = infer_request.infer_request.GetBlob(c_output_info_.begin()->first);
                            if (nullptr == outputBlob)
                            {
                                Logger::Print(LOG_LEVEL_ERROR, "Inference returned null pointer to output blob");
                                return SPEECH_LIBRARY_ERROR_GENERIC;
                            }
                            std::memcpy(output_index, outputBlob->buffer(),
                                infer_request.frames_to_compute_in_this_batch * output_vector_size() * sizeof(float));
                            output_index += infer_request.frames_to_compute_in_this_batch * output_vector_size();
                        }
                    }
                }

                if (frame_index == number_of_frames)
                {
                    infer_request.frame_index = -1;
                    continue;
                }
                if (c_input_info_.begin() == c_input_info_.end())
                {
                    Logger::Print(LOG_LEVEL_ERROR, "Inference has no single output info element");
                    return SPEECH_LIBRARY_ERROR_GENERIC;
                }
                else
                {
                    Blob::Ptr inputBlob = infer_request.infer_request.GetBlob(c_input_info_.begin()->first);
                    if (nullptr == inputBlob)
                    {
                        Logger::Print(LOG_LEVEL_ERROR, "Inference returned null pointer to input blob");
                        return SPEECH_LIBRARY_ERROR_GENERIC;
                    }

                    std::memcpy(inputBlob->buffer(), input_index,
                        frames_to_compute_in_this_batch * input_vector_size() * sizeof(float));

                    int index = static_cast<int>(frame_index) - (context_window_left_ + context_window_right_);
                    infer_request.infer_request.StartAsync();
                    infer_request.frame_index = index < 0 ? -2 : index;
                    infer_request.frames_to_compute_in_this_batch = frames_to_compute_in_this_batch;

                    frame_index += frames_to_compute_in_this_batch;

                    input_index += frames_to_compute_in_this_batch * input_vector_size();
                }
            }
        }
    }
    catch (const std::exception &error) {
        Logger::Print(LOG_LEVEL_ERROR, "%s", error.what());
        return SPEECH_LIBRARY_ERROR_GENERIC;
    }
    catch (...) {
        Logger::Print(LOG_LEVEL_ERROR, "Unknown/internal exception happened while processing data from InferenceEngine");
        return SPEECH_LIBRARY_ERROR_GENERIC;
    }
    return SPEECH_LIBRARY_SUCCESS;
}

SpeechLibraryStatus Scorer::Reset()
{
    // resetting state between utterances
    for (auto &&state : executable_network_.QueryState()) {
        state.Reset();
    }

    return SPEECH_LIBRARY_SUCCESS;
}
